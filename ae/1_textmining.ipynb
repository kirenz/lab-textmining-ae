{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Mining with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python setup\n",
    "\n",
    "We need the following modules in this notebook:\n",
    "\n",
    "- nltk\n",
    "- wordcloud\n",
    "- pandas\n",
    "- altair\n",
    "\n",
    "```bash\n",
    "activate mr\n",
    "```\n",
    "\n",
    "```bash\n",
    "pip install nltk wordcloud\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we suppress some unimportant warnings\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import some Tweets\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/kirenz/datasets/master/tweets-cnn.csv\")\n",
    "\n",
    "# drop some columns\n",
    "df.drop(columns=[\"author_id\", \"edit_history_tweet_ids\", \"id\"], inplace=True)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].astype(str).str.lower()\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text mining data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "\n",
    "- We use NLTK's [RegexpTokenizer](https://www.nltk.org/_modules/nltk/tokenize/regexp.html) to perform [tokenization](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization) in combination with regular expressions. \n",
    "\n",
    "- To learn more about regular expressions (\"regexp\"), visit the following sites:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- [regular expression basics](https://www.w3schools.com/python/python_regex.asp).\n",
    "- [interactive regular expressions tool](https://regex101.com/)\n",
    "\n",
    "\n",
    "- `\\w+` matches Unicode word characters with one or more occurrences; \n",
    "- this includes most characters that can be part of a word in any language, as well as numbers and the underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "\n",
    "regexp = RegexpTokenizer('___') # use regular expression to match (multiple) word characters and numbers\n",
    "\n",
    "df['text_token']=df['___'].apply(___.tokenize) # insert the data column and the regular expression pattern\n",
    "\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "regexp",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "regexp = RegexpTokenizer('\\w+')\n",
    "\n",
    "df['text_token']=df['text'].apply(regexp.tokenize)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "c-regexp",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check your code\n",
    "assert df.iloc[0, 2] == ['the',\n",
    " 'body',\n",
    " 'of',\n",
    " 'missing',\n",
    " 'princeton',\n",
    " 'university',\n",
    " 'student',\n",
    " 'misrach',\n",
    " 'ewunetie',\n",
    " 'has',\n",
    " 'been',\n",
    " 'found',\n",
    " 'https',\n",
    " 't',\n",
    " 'co',\n",
    " '66wv0od5ut']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare the entries of `text` with `text_token`. Do you notice any differences?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stopwords\n",
    "\n",
    "- Stop words are words in a stop list which are dropped before analysing natural language data since they don't contain valuable information (like \"will\", \"and\", \"or\", \"has\", ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# download the stopwords package\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Make a list of english stopwords\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# make your own custom stopwords\n",
    "my_stopwords = ['https', 'co']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Extend the stopword list with your own custom stopwords\n",
    "stopwords.extend(my_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Next, we use a [lambda function](https://www.w3schools.com/python/python_lambda.asp) (anonymous function) to remove the stopwords:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: \n",
    "\n",
    "We want to get rid of all stopwords in `text_token` and create a new column called `text_token_s` (for \"text token without stopwords\"). \n",
    "\n",
    "Therefore, we use the following code:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "df['text_token_s'] = df['text_token'].___(___ x: [__ for __ in x if __ not in ___])\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "You need to complete the code with the follwing information:\n",
    "\n",
    "\n",
    "- `.apply` applies a function along the rows of the DataFrame.\n",
    "\n",
    "\n",
    "- `lambda x:` is an anonymous funtion (we dont have to give it a name)\n",
    "\n",
    "\n",
    "- use `i` as iterator to iterate through every row and only keep words if they are not in `stopwords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "lambda",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "df['text_token_s'] = df['text_token'].apply(lambda x: [i for i in x if i not in stopwords])\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "c-lambda",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check your code\n",
    "assert df.iloc[1,3] == ['uk',\n",
    " 'prime',\n",
    " 'minister',\n",
    " 'liz',\n",
    " 'truss',\n",
    " 'quits',\n",
    " 'disastrous',\n",
    " 'six',\n",
    " 'weeks',\n",
    " 'office',\n",
    " 'putting',\n",
    " 'course',\n",
    " 'britain',\n",
    " 'shortest',\n",
    " 'serving',\n",
    " 'leader',\n",
    " '0o0xqscrxi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare the entries of `text_token_s` with `text_token`. Do you notice any differences?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transform data and remove infrequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the next step, we will:\n",
    "\n",
    "- transform the text tokens to a simple string (i.e. from cell value [a , b , c] to 'a b c') because the following steps (like lemmatization) can't handle tokens\n",
    "\n",
    "\n",
    "- remove words which occur less then two times (because such infrequent words usually don't have much value for our analysis)\n",
    "\n",
    "\n",
    "- save the result in a new column called `text_si` (`s` stands for stopword and `i` for infrequent words)\n",
    "\n",
    "\n",
    "\n",
    "Hint:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "___ = df['___'].___(lambda x: ' '.join([__ for __ in __ if len(__)>__]))\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "- name the new column `text_si`\n",
    "\n",
    "\n",
    "- use the column `text_token_s`\n",
    "\n",
    "\n",
    "- use `.apply` to apply a lambda function to every row of the dataframe\n",
    "\n",
    "\n",
    "- The lambda function should: \n",
    "\n",
    "  - combine (use `join()`) all word tokens (use `i` as an iterator) from a row in a single string (use a white space `' '`\n",
    " as seperator between the tokens)\n",
    "  - only keep tokens which occur more than 2 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "text_si",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "df['text_si'] = df['text_token_s'].apply(lambda x: ' '.join([i for i in x if len(i)>2]))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "c-text-si",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# check you code\n",
    "assert df.iloc[1, 4] == 'prime minister liz truss quits disastrous six weeks office putting course britain shortest serving leader 0o0xqscrxi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that this operation changes the format of your cell entries (notice the missing brackets). Do you notice further differences?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lemmatization\n",
    "\n",
    "- Next, we perform [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation) (lemmatization is the process of converting a word to its base form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# we need to download some packages\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create an object called wordnet_lem of the WordNetLemmatizer() function.\n",
    "wordnet_lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create a new column called text_sil (l for lemmatization) and apply the function .lemmatize\n",
    "df['text_sil'] = df['text_si'].apply(wordnet_lem.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we check wether there are any differences in the two columns\n",
    "check_difference = (df['text_sil'] == df['text_si'])\n",
    "\n",
    "# sum all True and False values\n",
    "check_difference.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can observe that on our data, the lemmatization function did not change an of the words (we have only `True` values, which means that every row in `df['text_sil'] == df['text_si']`).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"sentiment-cnn.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word cloud\n",
    "\n",
    "We use a word cloud to visualize our data ([word cloud example gallery](https://amueller.github.io/word_cloud/auto_examples/index.html#example-gallery))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# combine all words in one object called all_words\n",
    "all_words = ' '.join([i for i in df['text_sil']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(width=600, \n",
    "                     height=400, \n",
    "                     random_state=2, \n",
    "                     max_font_size=100).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Different style:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x, y = np.ogrid[:300, :300]\n",
    "mask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2\n",
    "mask = 255 * mask.astype(int)\n",
    "\n",
    "wc = WordCloud(background_color=\"white\", repeat=True, mask=mask)\n",
    "wc.generate(all_words)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Frequency distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# download the package\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize the words\n",
    "words_tokens = nltk.word_tokenize(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# use the function FreqDist and save the result as fd\n",
    "fd = FreqDist(words_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Most common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the 3 most common words by using the function `most_common(n=foo)` (foo is a placeholder).\n",
    "\n",
    "Use the object `fd` to obtain the result\n",
    "\n",
    "Save the result as `top_3`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "most_common",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# find the 3 most common words\n",
    "### BEGIN SOLUTION\n",
    "top_3 = fd.most_common(n=3)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "c-most-common",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check your code\n",
    "assert top_3 == [('trump', 5), ('president', 5), ('russian', 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# show the 3 most common words as table\n",
    "fd.tabulate(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plot common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Obtain top 10 words\n",
    "top_10 = fd.most_common(10)\n",
    "\n",
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# make a pandas datframe from the dictionary\n",
    "df_dist = pd.DataFrame({\"value\": dict(top_10)})\n",
    "\n",
    "df_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# reset index to transform index to column\n",
    "df_dist.reset_index(inplace=True)\n",
    "\n",
    "df_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "alt.Chart(df_dist).mark_bar().encode(\n",
    "    x=alt.X(\"value\"),\n",
    "    y=alt.Y(\"index\", sort=\"-x\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Search specific words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Show frequency of a specific word\n",
    "fd[\"trump\"]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "147de23312147813b0623f895fe208f672aacb131d01c672b840a7a22f97849e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
